{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4e322c6f-fab9-4003-9705-8c39fe43e800",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-3b02390c7677>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\downloader.py\u001b[0m in \u001b[0;36mdownload\u001b[1;34m(self, info_or_id, download_dir, quiet, force, prefix, halt_on_error, raise_on_error, print_error_to)\u001b[0m\n\u001b[0;32m    763\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mdownload_dir\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    764\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_download_dir\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdownload_dir\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 765\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_interactive_download\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    766\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    767\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\downloader.py\u001b[0m in \u001b[0;36m_interactive_download\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1113\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mTKINTER\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1114\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1115\u001b[1;33m                 \u001b[0mDownloaderGUI\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmainloop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1116\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mTclError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1117\u001b[0m                 \u001b[0mDownloaderShell\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\downloader.py\u001b[0m in \u001b[0;36mmainloop\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1932\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1933\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mmainloop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1934\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmainloop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1935\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1936\u001b[0m     \u001b[1;31m# /////////////////////////////////////////////////////////////////\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\tkinter\\__init__.py\u001b[0m in \u001b[0;36mmainloop\u001b[1;34m(self, n)\u001b[0m\n\u001b[0;32m   1427\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mmainloop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1428\u001b[0m         \u001b[1;34m\"\"\"Call the mainloop of Tk.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1429\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmainloop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1430\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1431\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mquit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# import nltk\n",
    "# nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1832c85-187f-45f4-bd1f-9125d5f08ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "# EXAMPLE_TEXT = \"Hello Mr. Smith, how are you doing today? The weather is great, and Python is awesome. The sky is pinkish-blue. You shouldn't eat cardboard.\"\n",
    "\n",
    "# print(sent_tokenize(EXAMPLE_TEXT))\n",
    "# print(word_tokenize(EXAMPLE_TEXT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af1b38e3-2acd-4c3b-b5d2-3e2f001d0d32",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d3a42690-efde-44f2-a981-524776be86d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # importing required modules\n",
    "# import PyPDF2\n",
    "  \n",
    "# # creating a pdf file object\n",
    "# pdfFileObj = open('example.pdf', 'rb')\n",
    "  \n",
    "# # creating a pdf reader object\n",
    "# pdfReader = PyPDF2.PdfFileReader(pdfFileObj)\n",
    "  \n",
    "# # printing number of pages in pdf file\n",
    "# print(pdfReader.numPages)\n",
    "  \n",
    "# # creating a page object\n",
    "# pageObj = pdfReader.getPage(0)\n",
    "\n",
    "# type(pageObj )\n",
    "# # extracting text from page\n",
    "# print(pageObj.extractText())\n",
    "  \n",
    "# # closing the pdf file object\n",
    "# pdfFileObj.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9ecb07dc-debc-4590-98b8-41a10e0f4036",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sent_tokenizer=nltk.data.load('tokenizers/punkt/portuguese.pickle')\n",
    "# sentences_tokenize = sent_tokenizer.tokenize(str(pageObj.extractText()))\n",
    "# palavras_tokenize = nltk.tokenize.word_tokenize(str(pageObj.extractText()), language='portuguese')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3926bfdd-9ae2-48b8-8969-adc8d4c0666a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14\n"
     ]
    }
   ],
   "source": [
    "# importing required modules\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import PyPDF2\n",
    "  \n",
    "# creating a pdf file object\n",
    "pdfFileObj = open('article.pdf', 'rb')\n",
    "  \n",
    "# creating a pdf reader object\n",
    "pdfReader = PyPDF2.PdfFileReader(pdfFileObj)\n",
    "  \n",
    "# printing number of pages in pdf file\n",
    "print(pdfReader.numPages)\n",
    "  \n",
    "# creating a page object\n",
    "pageObj = pdfReader.getPage(0)\n",
    "\n",
    "type(pageObj )\n",
    "# extracting text from page\n",
    "article_text=pageObj.extractText()\n",
    "  \n",
    "# closing the pdf file object\n",
    "pdfFileObj.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d4546a43-79e6-46d8-b28b-8953eea539e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "727"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "article_word_tokenized=word_tokenize(str(article_text))\n",
    "article_word_tokenized=[x.lower() for x in article_word_tokenized]\n",
    "#from collections import Counter\n",
    "#Counter(article_word_tokenized)\n",
    "len(article_word_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0f7f62b2-433b-43da-8413-18a453e17dd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "article_word_tokenized=[x for x in article_word_tokenized if not x in stopwords.words('english') ]\n",
    "len((article_word_tokenized))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "81185379-a61b-4a48-b5ba-efda8df12207",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "article_word_stemmed=[]\n",
    "\n",
    "for w in article_word_tokenized:\n",
    "    article_word_stemmed.append(ps.stem(w))\n",
    "    \n",
    "#set(article_word_tokenized)- set(article_word_stemmed)          \n",
    "#len(set(article_word_stemmed))\n",
    "article_word_stemmed=list(set(article_word_stemmed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "993955db-22ad-4e05-89fc-1c8414d929d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "article_word_post_tag=nltk.pos_tag(list(set((article_word_tokenized))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "444f14ff-1c05-480d-865b-2e5e97e98e08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "range(0, 10)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import glob\n",
    "article_list=glob.glob(\"corpus/*.pdf\")\n",
    "article_list=article_list[:10]\n",
    "range (len(article_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "112a19a2-215d-4d08-84f8-5da9d299f1f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "10\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xref table not zero-indexed. ID numbers for objects will be corrected.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n",
      "14\n",
      "12\n",
      "5\n",
      "7\n",
      "24\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_articles = []\n",
    "article_index = 0\n",
    "for article_index in range(len(article_list)):\n",
    "    pdfFileObj = open(article_list[article_index], 'rb')\n",
    "\n",
    "    # creating a pdf reader object\n",
    "    pdfReader = PyPDF2.PdfFileReader(pdfFileObj)\n",
    "    print (pdfReader.numPages)\n",
    "    article_text =[]\n",
    "    for i in range (pdfReader.numPages):\n",
    "        page = pdfReader.getPage(i)\n",
    "        article_text.append(page.extractText())\n",
    "\n",
    "\n",
    "    # closing the pdf file object\n",
    "    pdfFileObj.close()\n",
    "    base_articles.append(article_text)\n",
    "len(base_articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "ff0f4e4e-bd62-42d1-b6ec-5335e5c128e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from collections import Counter\n",
    "import re\n",
    "import string\n",
    "def preprocessing (article_text):\n",
    "    \n",
    "    article_word_tokenized=word_tokenize(str(article_text))\n",
    "    article_word_tokenized=[x.lower() for x in article_word_tokenized]\n",
    "    article_word_tokenized=[s.replace(\"\\\\n\", \"\") for s in article_word_tokenized]\n",
    "    article_word_tokenized=[x for x in article_word_tokenized if not x in stopwords.words('english')]\n",
    "    article_tokenized_stemmed=[]\n",
    "    for w in article_word_tokenized:\n",
    "        article_tokenized_stemmed.append(ps.stem(w))\n",
    "    article_tokenized_stemmed=list(set(article_tokenized_stemmed))\n",
    "    pattern = r'\\w'\n",
    "    article_tokenized_stemmed = [x for x in article_tokenized_stemmed if re.search(pattern, x)]\n",
    "    article_tokenized_stemmed=[s.translate(str.maketrans('', '', string.punctuation) for s in article_tokenized_stemmed)]\n",
    "    #article_tokenized_stemmed=([x for x in article_tokenized_stemmed if not x in [\"\",\".\",\",\",\"-\",\";\",\":\",\"?\",\"/\",\"\\\\\",\"...\",\"_\",\"=\",\"(\",\")\",\"[\",\"]\",\"{\",\"}\"]])\n",
    "    return Counter(article_tokenized_stemmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "dc0d6229-6a26-4782-b0d8-395bd9614ab0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'string With Punctuation'"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "036ff24c-c80e-46a1-b452-f7a017dead62",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'generator' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-89-d99754c623eb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0marticle_index\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0marticle_index\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbase_articles\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mbase_vocabulary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0marticle_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbase_articles\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mbase_vocabulary\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-88-84bfe6cbb98c>\u001b[0m in \u001b[0;36mpreprocessing\u001b[1;34m(article_text)\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[0mpattern\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34mr'\\w'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[0marticle_tokenized_stemmed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0marticle_tokenized_stemmed\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m     \u001b[0marticle_tokenized_stemmed\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranslate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmaketrans\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstring\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpunctuation\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[1;32min\u001b[0m \u001b[0marticle_tokenized_stemmed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m     \u001b[1;31m#article_tokenized_stemmed=([x for x in article_tokenized_stemmed if not x in [\"\",\".\",\",\",\"-\",\";\",\":\",\"?\",\"/\",\"\\\\\",\"...\",\"_\",\"=\",\"(\",\")\",\"[\",\"]\",\"{\",\"}\"]])\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mCounter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marticle_tokenized_stemmed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'generator' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "base_vocabulary = {}\n",
    "article_index = 0\n",
    "for article_index in range(len(base_articles)):\n",
    "    base_vocabulary[article_index]=dict(preprocessing(base_articles))\n",
    "base_vocabulary    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d6d4102a-5762-4316-86b8-b915f9ec3c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #faz o token das senten√ßas, para depois filtra-las conforme o formato desejado.\n",
    "# from nltk.tokenize import PunktSentenceTokenizer\n",
    "# custom_sent_tokenizer = PunktSentenceTokenizer(article_text)\n",
    "\n",
    "# tokenized = custom_sent_tokenizer.tokenize(article_text)\n",
    "# def process_content():\n",
    "#     try:\n",
    "#         for i in tokenized:\n",
    "#             words = nltk.word_tokenize(i)\n",
    "#             tagged = nltk.pos_tag(words)\n",
    "#             chunkGram = r\"\"\"Chunk: {<NNP.?>*<VB.?>*<NNP>+<NN>?}\"\"\"\n",
    "#             chunkParser = nltk.RegexpParser(chunkGram)\n",
    "#             chunked = chunkParser.parse(tagged)\n",
    "#             chunked.draw()     \n",
    "\n",
    "#     except Exception as e:\n",
    "#         print(str(e))\n",
    "\n",
    "# process_content()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
